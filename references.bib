@inproceedings{10.1145/3689061.3689075,
  author    = {Chao, Vanyi and Nguyen, Hoang Quoc and Jamsrandorj, Ankhzaya and Oo, Yin May and Mun, Kyung-Ryoul and Park, Hyowon and Park, Sangwon and Kim, Jinwook},
  title     = {Tracking the Blur: Accurate Ball Trajectory Detection in Broadcast Sports Videos},
  year      = {2024},
  isbn      = {9798400711985},
  publisher = {Association for Computing Machinery},
  address   = {New York, NY, USA},
  url       = {https://doi.org/10.1145/3689061.3689075},
  doi       = {10.1145/3689061.3689075},
  abstract  = {Ball trajectory data is one of the most fundamental and useful pieces of information for evaluating players' performance and analyzing game strategies in ball sports. Although vision-based object tracking techniques have been developed to analyze sports competition videos, it is still challenging to accurately recognize and position a high-speed, tiny ball in sports such as badminton, tennis, table tennis, and volleyball, among others. This is especially true in broadcasting videos, where the low frame rate causes high-velocity objects to appear blurry, sometimes disappear, and create afterimages. In this paper, a MaxVit Sequential model, based on the MaxVit architecture, is proposed to track tiny, high-velocity balls in sports broadcasting videos. To address low image quality issues such as blurriness, afterimages, and short-term occlusions, a sequential model that accepts a number of consecutive images is designed to detect these objects. Our approach is motivated by the practical challenges encountered during the manual annotation of ball positions; when the ball becomes indiscernible due to motion blur or occlusion, we frequently rely on adjacent frames to accurately infer its position. The experimental results demonstrate that the sequential frames model surpasses the single frame model in this task. The proposed model achieves superior performance across several metrics compared to baseline and state-of-the-art models. Specifically, our model attains an F1 score of 91.49, an accuracy of 85.41, an average precision of 91.16, and a recall of 91.82.},
  booktitle = {Proceedings of the 7th ACM International Workshop on Multimedia Content Analysis in Sports},
  pages     = {41–49},
  numpages  = {9},
  keywords  = {ball tracking, maxvit, vision transformer},
  location  = {Melbourne VIC, Australia},
  series    = {MMSports '24}
}

@misc{kovoweb,
  title = {Korea Volleyball Federation},
  year  = {2024},
  month = nov,
  note  = {[Online; accessed 1. Nov. 2024]},
  url   = {https://kovo.co.kr/KOVO}
}

@inproceedings{10312494,
  author    = {Oo, Yin May and Jamsrandorj, Ankhzaya and Chao, Vanyi and Mun, Kyung-Ryoul and Kim, Jinwook},
  booktitle = {IECON 2023- 49th Annual Conference of the IEEE Industrial Electronics Society},
  title     = {A Residual Attention-based EfficientNet Homography Estimation Model for Sports Field Registration},
  year      = {2023},
  volume    = {},
  number    = {},
  pages     = {1-7},
  keywords  = {Measurement;Industrial electronics;Deep learning;Image recognition;Tracking;Estimation;Logic gates;sports field registration;homography estimation;keypoints detection},
  doi       = {10.1109/IECON51785.2023.10312494}
}


@inproceedings{8578843,
  author    = {Hu, Jie and Shen, Li and Sun, Gang},
  booktitle = {2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  title     = {Squeeze-and-Excitation Networks},
  year      = {2018},
  volume    = {},
  number    = {},
  pages     = {7132-7141},
  keywords  = {Computer architecture;Computational modeling;Convolution;Task analysis;Convolutional codes;Adaptation models;Stacking},
  doi       = {10.1109/CVPR.2018.00745}
}

@inproceedings{10.5555/3295222.3295349,
  author    = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N. and Kaiser, \L{}ukasz and Polosukhin, Illia},
  title     = {Attention is all you need},
  year      = {2017},
  isbn      = {9781510860964},
  publisher = {Curran Associates Inc.},
  address   = {Red Hook, NY, USA},
  abstract  = {The dominant sequence transduction models are based on complex recurrent or convolutional neural networks that include an encoder and a decoder. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.0 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature.},
  booktitle = {Proceedings of the 31st International Conference on Neural Information Processing Systems},
  pages     = {6000–6010},
  numpages  = {11},
  location  = {Long Beach, California, USA},
  series    = {NIPS'17}
}

@misc{tdeed23,
  title         = {T-DEED: Temporal-Discriminability Enhancer Encoder-Decoder for Precise Event Spotting in Sports Videos},
  author        = {Artur Xarles and Sergio Escalera and Thomas B. Moeslund and Albert Clapés},
  year          = {2024},
  eprint        = {2404.05392},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CV},
  url           = {https://arxiv.org/abs/2404.05392}
}
@misc{spot22,
  title         = {Spotting Temporally Precise, Fine-Grained Events in Video},
  author        = {James Hong and Haotian Zhang and Michaël Gharbi and Matthew Fisher and Kayvon Fatahalian},
  year          = {2022},
  eprint        = {2207.10213},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CV},
  url           = {https://arxiv.org/abs/2207.10213}
}

@inproceedings{soccernetv2,
  title     = {Soccernet-v2: A dataset and benchmarks for holistic understanding of broadcast soccer videos},
  author    = {Deliege, Adrien and Cioppa, Anthony and Giancola, Silvio and Seikavandi, Meisam J and Dueholm, Jacob V and Nasrollahi, Kamal and Ghanem, Bernard and Moeslund, Thomas B and Van Droogenbroeck, Marc},
  booktitle = {Proceedings of the IEEE/CVF conference on computer vision and pattern recognition},
  pages     = {4508--4519},
  year      = {2021}
}

@inproceedings{soccernet,
  title     = {SoccerNet: A Scalable Dataset for Action Spotting in Soccer Videos},
  url       = {http://dx.doi.org/10.1109/CVPRW.2018.00223},
  doi       = {10.1109/cvprw.2018.00223},
  booktitle = {2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)},
  publisher = {IEEE},
  author    = {Giancola, Silvio and Amine, Mohieddine and Dghaily, Tarek and Ghanem, Bernard},
  year      = {2018},
  month     = jun
}


@misc{finediving,
  title         = {FineDiving: A Fine-grained Dataset for Procedure-aware Action Quality Assessment},
  author        = {Jinglin Xu and Yongming Rao and Xumin Yu and Guangyi Chen and Jie Zhou and Jiwen Lu},
  year          = {2022},
  eprint        = {2204.03646},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CV},
  url           = {https://arxiv.org/abs/2204.03646}
}

@inproceedings{figureskating,
  author    = {Hong, James and Fisher, Matthew and Gharbi, Michaël and Fatahalian, Kayvon},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision (ICCV)},
  title     = {Video Pose Distillation for Few-Shot, Fine-Grained Sports Action Recognition},
  year      = {2021},
  volume    = {},
  number    = {},
  pages     = {9234-9243},
  keywords  = {Visualization;Target recognition;Impedance matching;Pose estimation;Detectors;Feature extraction;Pattern recognition;Transfer/Low-shot/Semi/Unsupervised Learning;Action and behavior recognition;Gestures and body pose;Representation learning;Video analysis and understanding},
  doi       = {10.1109/ICCV48922.2021.00912}
}

@inproceedings{tsn,
  author    = {Wang, Limin
               and Xiong, Yuanjun
               and Wang, Zhe
               and Qiao, Yu
               and Lin, Dahua
               and Tang, Xiaoou
               and Van Gool, Luc},
  editor    = {Leibe, Bastian
               and Matas, Jiri
               and Sebe, Nicu
               and Welling, Max},
  title     = {Temporal Segment Networks: Towards Good Practices for Deep Action Recognition},
  booktitle = {Computer Vision -- ECCV 2016},
  year      = {2016},
  publisher = {Springer International Publishing},
  address   = {Cham},
  pages     = {20--36},
  abstract  = {Deep convolutional networks have achieved great success for visual recognition in still images. However, for action recognition in videos, the advantage over traditional methods is not so evident. This paper aims to discover the principles to design effective ConvNet architectures for action recognition in videos and learn these models given limited training samples. Our first contribution is temporal segment network (TSN), a novel framework for video-based action recognition. which is based on the idea of long-range temporal structure modeling. It combines a sparse temporal sampling strategy and video-level supervision to enable efficient and effective learning using the whole action video. The other contribution is our study on a series of good practices in learning ConvNets on video data with the help of temporal segment network. Our approach obtains the state-the-of-art performance on the datasets of HMDB51 ({\$}{\$} 69.4{\backslash},{\backslash}{\%} {\$}{\$}) and UCF101 ({\$}{\$} 94.2{\backslash},{\backslash}{\%} {\$}{\$}). We also visualize the learned ConvNet models, which qualitatively demonstrates the effectiveness of temporal segment network and the proposed good practices (Models and code at https://github.com/yjxiong/temporal-segment-networks).},
  isbn      = {978-3-319-46484-8}
}

@inproceedings{8578773,
  author    = {Tran, Du and Wang, Heng and Torresani, Lorenzo and Ray, Jamie and LeCun, Yann and Paluri, Manohar},
  booktitle = {2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  title     = {A Closer Look at Spatiotemporal Convolutions for Action Recognition},
  year      = {2018},
  volume    = {},
  number    = {},
  pages     = {6450-6459},
  keywords  = {Three-dimensional displays;Two dimensional displays;Spatiotemporal phenomena;Solid modeling;Feature extraction;Computer architecture},
  doi       = {10.1109/CVPR.2018.00675}
}

@misc{zhou2018temporalrelationalreasoningvideos,
  title         = {Temporal Relational Reasoning in Videos},
  author        = {Bolei Zhou and Alex Andonian and Aude Oliva and Antonio Torralba},
  year          = {2018},
  eprint        = {1711.08496},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CV},
  url           = {https://arxiv.org/abs/1711.08496}
}

@misc{radosavovic2020designingnetworkdesignspaces,
  title         = {Designing Network Design Spaces},
  author        = {Ilija Radosavovic and Raj Prateek Kosaraju and Ross Girshick and Kaiming He and Piotr Dollár},
  year          = {2020},
  eprint        = {2003.13678},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CV},
  url           = {https://arxiv.org/abs/2003.13678}
}


@inproceedings{9156729,
  author    = {Sudhakaran, Swathikiran and Escalera, Sergio and Lanz, Oswald},
  booktitle = {2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
  title     = {Gate-Shift Networks for Video Action Recognition},
  year      = {2020},
  volume    = {},
  number    = {},
  pages     = {1099-1108},
  keywords  = {Two dimensional displays;Three-dimensional displays;GSM;Convolution;Feature extraction;Logic gates;Kernel},
  doi       = {10.1109/CVPR42600.2020.00118}
}


@inproceedings{9578330,
  author    = {Z. Qing and H. Su and W. Gan and D. Wang and W. Wu and X. Wang and Y. Qiao and J. Yan and C. Gao and N. Sang},
  booktitle = {2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
  title     = {Temporal Context Aggregation Network for Temporal Action Proposal Refinement},
  year      = {2021},
  volume    = {},
  issn      = {},
  pages     = {485-494},
  abstract  = {Temporal action proposal generation aims to estimate temporal intervals of actions in untrimmed videos, which is a challenging yet important task in the video understanding field. The proposals generated by current methods still suffer from inaccurate temporal boundaries and inferior confidence used for retrieval owing to the lack of efficient temporal modeling and effective boundary context utilization. In this paper, we propose Temporal Context Aggregation Network (TCANet) to generate high-quality action proposals through &quot;local and global&quot; temporal context aggregation and complementary as well as progressive boundary refinement. Specifically, we first design a Local-Global Temporal Encoder (LGTE), which adopts the channel grouping strategy to efficiently encode both &quot;local and global&quot; temporal inter-dependencies. Furthermore, both the boundary and internal context of proposals are adopted for frame-level and segment-level boundary regressions, respectively. Temporal Boundary Regressor (TBR) is designed to combine these two regression granularities in an end-to-end fashion, which achieves the precise boundaries and reliable confidence of proposals through progressive refinement. Extensive experiments are conducted on three challenging datasets: HACS, ActivityNet-v1.3, and THUMOS-14, where TCANet can generate proposals with high precision and recall. By combining with the existing action classifier, TCANet can obtain remarkable temporal action detection performance compared with other methods. Not surprisingly, the proposed TCANet won the 1st place in the CVPR 2020 - HACS challenge leaderboard on temporal action localization task.},
  keywords  = {location awareness;computer vision;benchmark testing;reliability engineering;pattern recognition;proposals;task analysis},
  doi       = {10.1109/CVPR46437.2021.00055},
  url       = {https://doi.ieeecomputersociety.org/10.1109/CVPR46437.2021.00055},
  publisher = {IEEE Computer Society},
  address   = {Los Alamitos, CA, USA},
  month     = {jun}
}

@inproceedings{10.1007/978-3-319-46487-9_47,
  author    = {Escorcia, Victor
               and Caba Heilbron, Fabian
               and Niebles, Juan Carlos
               and Ghanem, Bernard},
  editor    = {Leibe, Bastian
               and Matas, Jiri
               and Sebe, Nicu
               and Welling, Max},
  title     = {DAPs: Deep Action Proposals for Action Understanding},
  booktitle = {Computer Vision -- ECCV 2016},
  year      = {2016},
  publisher = {Springer International Publishing},
  address   = {Cham},
  pages     = {768--784},
  abstract  = {Object proposals have contributed significantly to recent advances in object understanding in images. Inspired by the success of this approach, we introduce Deep Action Proposals (DAPs), an effective and efficient algorithm for generating temporal action proposals from long videos. We show how to take advantage of the vast capacity of deep learning models and memory cells to retrieve from untrimmed videos temporal segments, which are likely to contain actions. A comprehensive evaluation indicates that our approach outperforms previous work on a large scale action benchmark, runs at 134 FPS making it practical for large-scale scenarios, and exhibits an appealing ability to generalize, i.e. to retrieve good quality temporal proposals of actions unseen in training.},
  isbn      = {978-3-319-46487-9}
}

@inproceedings{10203543,
  author    = {Shi, Dingfeng and Zhong, Yujie and Cao, Qiong and Ma, Lin and Lit, Jia and Tao, Dacheng},
  booktitle = {2023 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
  title     = {TriDet: Temporal Action Detection with Relative Boundary Modeling},
  year      = {2023},
  volume    = {},
  number    = {},
  pages     = {18857-18866},
  keywords  = {Convolutional codes;Computer vision;Computational modeling;Aggregates;Benchmark testing;Probability distribution;Pattern recognition;Video: Action and event understanding},
  doi       = {10.1109/CVPR52729.2023.01808}
}


@inproceedings{zhang2022actionformer,
  title     = {ActionFormer: Localizing Moments of Actions with Transformers},
  author    = {Zhang, Chen-Lin and Wu, Jianxin and Li, Yin},
  booktitle = {European Conference on Computer Vision},
  series    = {LNCS},
  volume    = {13664},
  pages     = {492-510},
  year      = {2022}
}

@inproceedings{tridet,
  author    = {Shi, Dingfeng and Zhong, Yujie and Cao, Qiong and Ma, Lin and Lit, Jia and Tao, Dacheng},
  booktitle = {2023 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
  title     = {TriDet: Temporal Action Detection with Relative Boundary Modeling},
  year      = {2023},
  volume    = {},
  number    = {},
  pages     = {18857-18866},
  keywords  = {Convolutional codes;Computer vision;Computational modeling;Aggregates;Benchmark testing;Probability distribution;Pattern recognition;Video: Action and event understanding},
  doi       = {10.1109/CVPR52729.2023.01808}
}


@article{9171561,
  author   = {Yang, Le and Peng, Houwen and Zhang, Dingwen and Fu, Jianlong and Han, Junwei},
  journal  = {IEEE Transactions on Image Processing},
  title    = {Revisiting Anchor Mechanisms for Temporal Action Localization},
  year     = {2020},
  volume   = {29},
  number   = {},
  pages    = {8535-8548},
  keywords = {Pipelines;Proposals;Videos;Task analysis;Detectors;Object detection;Automation;Temporal action localization;default anchor;anchor free;complementarity},
  doi      = {10.1109/TIP.2020.3016486}
}

@inproceedings{ActivityNet,
  author = {Heilbron, Fabian and Escorcia, Victor and Ghanem, Bernard and Niebles, Juan Carlos},
  year   = {2015},
  month  = {06},
  pages  = {},
  title  = {ActivityNet: A Large-Scale Video Benchmark for Human Activity Understanding},
  doi    = {10.1109/CVPR.2015.7298698}
}

@misc{damen2018scalingegocentricvisionepickitchens,
  title         = {Scaling Egocentric Vision: The EPIC-KITCHENS Dataset},
  author        = {Dima Damen and Hazel Doughty and Giovanni Maria Farinella and Sanja Fidler and Antonino Furnari and Evangelos Kazakos and Davide Moltisanti and Jonathan Munro and Toby Perrett and Will Price and Michael Wray},
  year          = {2018},
  eprint        = {1804.02748},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CV},
  url           = {https://arxiv.org/abs/1804.02748}
}

@article{Idrees_2017,
  title     = {The THUMOS challenge on action recognition for videos “in the wild”},
  volume    = {155},
  issn      = {1077-3142},
  url       = {http://dx.doi.org/10.1016/j.cviu.2016.10.018},
  doi       = {10.1016/j.cviu.2016.10.018},
  journal   = {Computer Vision and Image Understanding},
  publisher = {Elsevier BV},
  author    = {Idrees, Haroon and Zamir, Amir R. and Jiang, Yu-Gang and Gorban, Alex and Laptev, Ivan and Sukthankar, Rahul and Shah, Mubarak},
  year      = {2017},
  month     = feb,
  pages     = {1–23}
}

