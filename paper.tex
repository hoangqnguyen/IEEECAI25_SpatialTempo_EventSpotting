\documentclass[conference]{IEEEtran}
\IEEEoverridecommandlockouts
% The preceding line is only needed to identify funding in the first footnote. If that is unneeded, please comment it out.
\usepackage{cite}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{textcomp}
\usepackage{xcolor}
\usepackage{hyperref}

\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}
\begin{document}

\title{DMTN: Dual-Attention Multi-Task Network for Precise Spatio-Temporal Event Spotting in Volleyball Videos\\
% \thanks{Identify applicable funding agency here. If none, delete this.}
}

\author{\IEEEauthorblockN{1\textsuperscript{st} Hoang Quoc Nguyen}
\IEEEauthorblockA{\textit{AI Robotics} \\
\textit{University of Science and Technology}\\
Daejeon, 34113, Republic of Korea \\
nqhoang@ust.ac.kr}
\and
\IEEEauthorblockN{2\textsuperscript{nd} Ankhzaya Jamsrandorj}
\IEEEauthorblockA{\textit{Human Computer Interface and Robotics} \\
\textit{University of Science and Technology}\\
Daejeon, 34113, Republic of Korea \\
ankhaa@ust.ac.kr}
\and
\IEEEauthorblockN{3\textsuperscript{rd} Vanyi Chao}
\IEEEauthorblockA{\textit{AI Robotics} \\
\textit{University of Science and Technology}\\
Daejeon, 34113, Republic of Korea \\
vanyi@ust.ac.kr}
\and
\IEEEauthorblockN{4\textsuperscript{th} Yin May Oo}
\IEEEauthorblockA{\textit{AI Robotics} \\
\textit{University of Science and Technology}\\
Daejeon, 34113, Republic of Korea \\
yinmay@ust.ac.kr}
\and
\IEEEauthorblockN{5\textsuperscript{th} Kyung-Ryoul Mun}
\IEEEauthorblockA{\textit{Center for Artificial Intelligence} \\
\textit{Korea Institute of Science and Technology}\\
Seoul, 02792, Republic of Korea \\
krmoon02@kist.re.kr}
\and
\IEEEauthorblockN{6\textsuperscript{th} Jinwook Kim}
\IEEEauthorblockA{\textit{Center for Artificial Intelligence} \\
\textit{Korea Institute of Science and Technology}\\
Seoul, 02792, Republic of Korea \\
jinwook.kim21@gmail.com}
}

\maketitle

\begin{abstract}
    In sport videos analysis, especially in speedy sports such as Volleyball, understanding and localizing the precise timing and location of actions and events are critical. We introduce a new task: Precise Spatio-Temporal Event Spotting, which aims to detect both when and where key events happen. To support this, we develop the KOVO Volleyball Event Dataset, featuring 947 rally videos, and 5,935 events, annotated for both temporal and spatial localization. Our best model achieves a combined mAP of 85.46 across various temporal and spatial thresholds.
    % TODO: This part should compare results of SPOT and TDEED on KOVO
    %  Notably, we find that incorporating spatial predictions enhances temporal mAP by 5.89 points, underscoring the synergy between spatial and temporal analysis. 
     To the best of our knowledge, this is the first work addressing this task, establishing a strong baseline for future research in spatio-temporal event spotting.
\end{abstract}

\begin{IEEEkeywords}
precise event spotting, video understanding, spatial temporal event spotting, deep learning, volleyball, sport
\end{IEEEkeywords}

\section{Introduction}

Video understanding has emerged as a cornerstone in computer vision, offering valuable insights into dynamic scenes for applications such as sports analytics, surveillance, and autonomous systems. This field encompasses various tasks designed to interpret and analyze actions over time. Among these, \textit{Video Classification} aims to assign a single label to an entire video, providing a broad understanding of the content but often lacking frame-level precision. 
In contrast, \textit{Temporal Action Localization (TAL)} centers on pinpointing time intervals where specific actions take place within untrimmed videos. Complementing TAL, \textit{Precise Action Spotting (PES)} seeks to identify the exact frames that capture crucial events, requiring models to recognize subtle temporal distinctions and discern visually similar frames \cite{spot22}.

Recent developments in action spotting, like \textit{T-DEED} \cite{tdeed23} and \textit{E2E-Spot} \cite{spot22}, have shown that models can achieve frame-level accuracy in capturing fast-paced events by leveraging deep learning architectures. Datasets such as \textit{FigureSkating} \cite{figureskating} and \textit{FineDiving} \cite{finediving} have driven progress in action spotting, particularly highlighting the need for precise temporal detection in sports focused on individual athletes. However, these datasets are narrowly focused on specific sports and do not reflect the rapid interactions and complexities seen in high-speed, team-oriented sports like volleyball.

In volleyball, quick transitions play out in particular regions of the court, making spatial localization just as critical as temporal precision. To address this gap, we introduce a new task: \textit{Precise Spatio-Temporal Event Spotting} (as depicted in Fig.~\ref{fig-kovo-ex}), which targets both the exact timing and spatial positioning of key events. This approach, unlike traditional action spotting, aims to provide deeper insights into player positioning and movement patterns, which are essential for analyzing volleyball gameplay.

For other sports, datasets like \textit{SoccerNet-v2} \cite{soccernetv2} have advanced action spotting with detailed temporal and spatial annotations, helping push model capabilities forward. Yet, no similar dataset exists for volleyball, a sport characterized by its rapid exchanges and high demand for spatial precision. To bridge this gap, we present the \textit{KOVO Event Dataset}, which consists of 947 rally videos, 890,797 frames, and 5,935 annotated key actions. This dataset provides fine-grained annotations for both temporal and spatial event localization, making it an invaluable resource for building models that capture volleyball's unique dynamics.

\begin{figure*}[t] 
    \centering
    \includegraphics[width=\textwidth]{figures/fig-kovo-ex.jpg}
    \caption{Example sequence for a spike event, highlighting the exact timing (\(t+3\)) with the spatial location annotated by the white circle and the event class indicated by the text above it.}
    \label{fig-kovo-ex}
\end{figure*}



Our contributions are threefold:
\begin{itemize}
    \item \textbf{New Task Introduction:} We define the task of Precise Spatio-Temporal Event Spotting, tailored specifically to capture the rapid dynamics of volleyball.
    \item \textbf{Dataset Development:} We introduce the \textit{KOVO Event Dataset}, the first dataset to feature detailed temporal and spatial annotations for volleyball rallies, intended to drive research in this domain.
    \item \textbf{Model Development:} We propose a multi-task deep learning model that jointly predicts event timing and spatial locations, using this dual focus to achieve better performance.
    % Notably, incorporating spatial predictions into our model enhances temporal mAP by 5.89 points.
\end{itemize}

Our best model achieves a temporal mAP of 90.59, a spatial mAP of 77.94, and a combined mAP of 85.46, providing a strong baseline for this new task. To the best of our knowledge, this work is the first to explore Precise Spatio-Temporal Event Spotting in volleyball, paving the way for future research in this field.


% TODO: Fill this when finish paper
% The paper proceeds with a discussion of related work in Section 2, a detailed description of our approach in Section 3, the experimental setup in Section 4, results in Section 5, and conclusions in Section 6.


\section{Related work}

\subsection{Video Classification}

Video classification aims to predict a single label for an entire video, in contrast to event spotting, which requires precise frame-level labeling. This distinction introduces unique challenges: video classification can leverage sparse frame sampling \cite{tsn}, whereas event spotting demands dense sampling to capture rapid changes in events. Additionally, classification models often employ global space-time pooling \cite{8578773} or temporal consensus \cite{zhou2018temporalrelationalreasoningvideos} to produce video-level predictions, while event spotting necessitates preserving high temporal resolution.


\subsection{Temporal Action Localization}

The goal of Temporal Action Localization (TAL) is to localize the temporal intervals when specific actions occur in untrimmed videos, making it ideal for prolonged actions that are not instantaneous. In contrast to video classification, which assigns a single label to an entire video, TAL requires precise start and end times, adding more complexity.

Typically, there are two approaches to TAL: two-stage \cite{9578330,10.1007/978-3-319-46487-9_47} and one-stage \cite{10203543,zhang2022actionformer}. In two-stage models, action proposals are generated first then used for classification, while action class and temporal intervals are done in one streamlined process. Recently, advanced techniques such as Transformers and Feature Pyramid Network are incorporated to improve temporal precision across varying action duractions, like ActionFormer \cite{zhang2022actionformer} and TriDet \cite{tridet}. Anchor-free approaches \cite{9171561} have further enhanced flexibility in predicting actions without relying on predefined time windows.

Datasets and benchmarks such as ActivityNet \cite{ActivityNet}, EPIC-KITCHENS \cite{damen2018scalingegocentricvisionepickitchens}, and THUMOS Challenge \cite{Idrees_2017} have driven the development of TAL, which has become a central topic in Video Understand. By contrast, the precise identification of single-timestamp events remains less explored.

\subsection{Precise Event Spotting}

Precise Event Spotting (PES) aims to detect the exact frames of key events in untrimmed videos, making it ideal for fast, critical moments in sports. Unlike Temporal Action Localization (TAL), which spans broader time intervals, PES requires frame-level accuracy, crucial for detailed sports analysis where slight timing shifts can alter game interpretations.

Datasets like \textit{FigureSkating} \cite{figureskating} and \textit{FineDiving} \cite{finediving} have advanced PES with frame-level annotations, but focus on simpler, individual sports. Recent methods, such as \textit{T-DEED} \cite{tdeed23} and \textit{E2E-Spot} \cite{spot22}, enhance precision by refining temporal representations and avoiding pooling, but lack spatial context needed for team sports.

In volleyball, spatial localization is as critical as timing. Understanding event locations is key to analyzing player movements and strategies. Our work addresses this by introducing a new task and dataset for Precise Spatio-Temporal Event Spotting in volleyball, capturing both event timing and location.

Inspired by \textit{E2E-Spot} \cite{spot22}, our approach uses RegNet-Y \cite{radosavovic2020designingnetworkdesignspaces} with GSM \cite{9156729} for adaptive temporal shifts. This combination balances efficiency and precision, making it ideal for capturing complex spatio-temporal dynamics in volleyball.

\section{Dataset Overview} With permission from the Korea Volleyball Federation (KOVO) \cite{kovoweb}, we collected 235 volleyball game videos, each containing temporal annotations for game events. Using field keypoint detection \cite{10312494}, we segmented each game into "replay" and "in-play" sequences (rallies). Each rally was then converted to individual frames at 1280x720 resolution and 30 fps. To facilitate event location annotation, we developed a proprietary tool, further detailed in the Annotation Process subsection.

% \subsection{Data Content and Statistics} To automate data collection, we created a Python script to download game videos and event timings. This data, once segmented into rallies and converted to frames, was annotated using our custom tool, designed to streamline collaboration and expedite labeling. Further details on this process are outlined in the Annotation Process subsection.

\subsection{Annotation Process}

Each event in the dataset is annotated with precise spatial information, specifically focusing on the ball’s location, as this serves as the primary indicator for volleyball actions. To achieve high accuracy, we manually labeled the ball’s position in each frame, ensuring detailed and reliable annotations essential for understanding the dynamics of each event. Although this approach is time-intensive, it provides high-quality data that underpins robust event detection and tracking.

The dataset encompasses six core volleyball actions—Serve, Receive, Set, Spike, Dig, and Block—provided by the Korea Volleyball Federation (KOVO). These actions capture the key elements of gameplay and balance both offensive and defensive actions critical to rally sequences.

To streamline future work, advanced models like MaxVit Sequential \cite{10.1145/3689061.3689075} could assist in automating the ball-tracking process, reducing manual effort while maintaining annotation quality.


\begin{table}[htbp]
    \caption{KOVO Dataset Action Counts and Percentages}
    \begin{center}
    \begin{tabular}{|c|c|c|}
    \hline
    \textbf{Action} & \textbf{\# Events} & \textbf{Percentage (\%)} \\
    \hline
    Serve & 947 & 15.96 \\
    Receive & 844 & 14.22 \\
    Set & 1359 & 22.90 \\
    Spike & 1315 & 22.16 \\
    Dig & 807 & 13.60 \\
    Block & 663 & 11.17 \\
    \hline
    \end{tabular}
    \label{tab-kovo-stat1}
    \end{center}
    \end{table}
    

    \subsection{Dataset Splits}

    We divided the KOVO Event Dataset into three distinct subsets: training, validation, and testing. These subsets were selected to reflect a balanced representation of rally variations, camera angles, and visual characteristics across different matches, which helps improve model generalization.
    
    The split ratios are as follows: the training set comprises 561 rallies (approximately 60\% of the dataset), the validation set includes 220 rallies (around 23\%), and the testing set contains 166 rallies (about 17.5\%). 
    

\begin{table}[htbp]
    \caption{Dataset Splits for Volleyball Analysis}
    \begin{center}
    \begin{tabular}{|c|c|c|c|}
    \hline
    \textbf{Split} & \textbf{\# Clips (Rallies)} & \textbf{\# Events} & \textbf{\# Frames} \\
    \hline
    All & 947 & 5,935 & 257,388 \\
    Train & 561 & 3,631 & 156,818 \\
    Validation & 220 & 1,356 & 58,430 \\
    Test & 166 & 948 & 42,140 \\
    \hline
    \end{tabular}
    \label{tab-kovo-split}
    \end{center}
\end{table}

    
\subsection{Release and Access}

The full-resolution dataset is unavailable due to size constraints, but a resized version (512x288) with annotations (100GB) is accessible on Kaggle at \url{www.kaggle.com/soon}. Access requires a completed NDA form due to copyright restrictions.

\section{Proposed Method}
\subsection{Problem Formulation}
\textbf{Precise Spatio-Temporal Event Spotting (PSTES)} aims to identify both the precise time and location of events within untrimmed videos. For a sequence of \(N\) frames \(x_1, \ldots, x_N\) and a set of \(K\) event classes \(c_1, \ldots, c_K\), the goal is to predict a sparse set of frame indices where events occur, as well as the corresponding event class and spatial coordinates. Each prediction is represented as \((t, c_t, s_t)\), where \(t\) is the frame index, \(c_t\) is the predicted event class, and \(s_t\) is the spatial location of the event within that frame. A temporal prediction is deemed correct if it falls within a small temporal tolerance \(\sigma_f\) frames of the labeled event and matches the ground-truth class. Similarly, spatial predictions are deemed correct if the distance between the predicted location \(s_t\) and the ground-truth event location is within a specified threshold \(\sigma_p\). For PSTES, these tolerances are kept small, requiring precise frame-level accuracy, and it assumes that videos are captured with sufficiently high frame rates.

To achieve strong performance on this task, several important factors must be considered:

\begin{itemize}
    \item \textbf{Capturing Local Spatio-Temporal Features}: The model must be able to detect subtle visual changes and movements across adjacent frames, which is crucial for differentiating between events that may look quite similar.
    \item \textbf{Incorporating Long-Term Temporal Context}: By analyzing a wider temporal range, the model can better understand how events unfold over time, such as tracking how player movements or actions develop before and after a key moment.
    \item \textbf{Producing Dense Frame-Level Predictions}: For each frame \(x_t\), the model outputs predictions \((\hat{y}_t, \hat{s}_t)\), where \(\hat{y}_t \in \mathbb{R}^K\) represents the class probabilities in a vector of logits, and \(\hat{s}_t \in \mathbb{R}^2\) provides the spatial coordinates \((x, y)\) of the event in the frame. The event class \(c_t\) is then determined by applying argmax to \(\hat{y}_t\).
\end{itemize}

Meeting these requirements necessitates a robust, end-to-end architecture that effectively leverages both spatial and temporal data. Our approach employs a sequence model that integrates local spatio-temporal features to achieve this goal.


\subsection{Model Architecture}
To effectively learn spatio-temporal features, we propose a multi-task deep learning model comprising three key components: a Feature Extractor, a Dual-attention Feature Aggregation module, and Spatio-Temporal Prediction Heads. Drawing inspiration from Hong et al.\cite{spot22}, our model utilizes a RegNet-Y backbone \cite{radosavovic2020designingnetworkdesignspaces} combined with a Gate Shift Module (GSM) \cite{9156729} to adaptively shift temporal features for each input frame. These features are then aggregated through a dual-attention mechanism that merges spatial and temporal information, enhancing the representation of features. The model outputs event classifications and spatial coordinates using separate heads, each trained with a multi-task loss function that balances both temporal and spatial prediction goals.

% TODO: Add a Math formula for the model architecture: ...

\begin{figure*} 
    \centering
    \includegraphics[width=\textwidth]{figures/fig-arch.jpg}
    \caption{Model architecture.}
    \label{fig-arch}
\end{figure*}



\subsubsection{Feature Extractor}
The Feature Extractor processes each frame \( x_t \in \mathbb{R}^{H \times W \times 3} \) using a RegNet-Y backbone integrated with a Gate Shift Module (GSM) to capture both spatial and temporal information at once. With GSM embedded in RegNet-Y, the network learns not only spatial details but also temporal motion between frames, as GSM adaptively shifts features to track longer-term dependencies. This integration produces a spatio-temporal feature \( f_t \in \mathbb{R}^d \) that captures both motion and spatial characteristics, equipping the model to interpret event flow and predict its progression accurately.

\subsubsection{Dual-Attention Feature Aggregation}
The Dual-Attention Feature Aggregation consists of layers combining spatial and temporal attention to improve the model’s recognition of complex event dynamics. By integrating a Channel Attention Module and a Temporal Attention Module within each layer, this setup enables the model to emphasize the most informative spatial and temporal features. The dual-attention mechanism selectively focuses on frames and features essential for classification, allowing the model to capture critical moments and transitions precisely, which is especially important in fast-paced scenarios.

\begin{figure}[htbp]
    \centerline{\includegraphics{figures/fig1.png}}
    \caption{Dual-Attention Feature Aggregation Block.}
    \label{fig2}
\end{figure}

\begin{itemize}
    \item \textbf{Channel Attention Module:} This module uses the Squeeze-and-Excitation (SE) mechanism \cite{8578843} for channel-level recalibration, bringing forward the most informative channels within each frame. By focusing on key spatial regions, it helps the model differentiate visually similar frames. In fast-paced sports like volleyball, this allows the model to recognize subtle differences, such as distinguishing a spike attempt from a feint or a quick switch from blocking to defensive posture, which may look alike but signify different actions.

    \item \textbf{Temporal Attention Module:} This encoder-only module uses multi-head attention \cite{10.5555/3295222.3295349} across frames to capture dependencies over time and the flow of events. By selectively focusing on frames that most contribute to classification, rather than treating all frames equally, the Temporal Attention Module enhances the model’s ability to pinpoint key moments and accurately understand event timing and dynamics in fast-paced sequences.
\end{itemize}


\subsubsection{Spatio-Temporal Prediction Heads}

After the Dual-Attention Aggregation Block, the features are passed into two separate prediction heads in parallel: one for predicting the spatial location of events and the other for identifying event classes. Each head is optimized with a multi-task loss function to balance both temporal and spatial prediction objectives effectively.

\begin{itemize}
    \item \textbf{Event Class Head:} This head outputs a vector of classification logits \(\hat{y}_t\), which represents the probabilities for each possible event class. The predicted event class \(\hat{C}\) is obtained by applying the \(\arg\max\) function:
    \[
    \hat{C} = \arg\max(\hat{y}_t)
    \]

    \item \textbf{Spatial Location Head:} This head outputs logits for the spatial location of the event, represented as coordinates \(\hat{s}_t\). To obtain a normalized value between 0 and 1, a sigmoid function is applied to the location logits, resulting in the spatial location \(\hat{L}\):
    \[
    \hat{L} = \sigma(\hat{s}_t)
    \]
    where \(\sigma\) denotes the sigmoid function, ensuring that \(\hat{L}\) is expressed as a proportion of the frame dimensions.
\end{itemize}

\subsubsection{Multi-task Loss Function}

The model is trained using a multi-task loss function to balance the objectives of temporal event classification and spatial location prediction, enabling the model to learn both when and where key events occur. The total loss is the sum of two components: a temporal classification loss for event class predictions and a spatial regression loss for location accuracy. These are defined as follows:

\begin{itemize}
    \item \textbf{Temporal Classification Loss (Cross-Entropy):} The event class prediction is supervised with a cross-entropy loss, \(\mathcal{L}_{\text{class}}\), which includes an additional background class (class 0) for non-event frames. Due to the sparsity of event frames relative to background frames, a weighting strategy is applied to emphasize event frames more heavily during training. Following the approach in \textit{E2E-Spot} \cite{spot22}, we assign a weight of 5 to event classes and a weight of 1 to the background class. The weighted cross-entropy loss is defined as:
    \[
    \mathcal{L}_{\text{class}} = -\sum_{c} w_c \, y_{c} \, \log(\hat{y}_{c})
    \]
    where \(w_c\) is the class weight (5 for event classes, 1 for the background), \(y_c\) is the ground-truth label, and \(\hat{y}_{c}\) is the predicted probability for each class \(c\).

    \item \textbf{Spatial Location Loss (L1):} The spatial prediction is supervised with an L1 loss, \(\mathcal{L}_{\text{spatial}}\), which calculates the absolute difference between the predicted location \(\hat{L}\) and the ground truth location \(L\). A binary mask \(M\) is applied to focus the loss on frames where the class label is greater than 0 (i.e., frames containing events), thus avoiding gradient backpropagation of spatial predictions on background frames:
    \[
    \mathcal{L}_{\text{spatial}} = \frac{1}{\sum M + \epsilon} \sum_{t} M_t \, |\hat{L}_t - L_t|
    \]
    where \(M_t\) is 1 for event frames (class label $y_c > 0$) and 0 for background frames ($y_c = 0$), and \(\epsilon\) is a small constant for numerical stability.

\end{itemize}

The total multi-task loss, combining both the temporal classification and spatial location losses, is defined as:
\[
\mathcal{L} = \mathcal{L}_{\text{class}} + \mathcal{L}_{\text{spatial}}
\]

By jointly optimizing \(\mathcal{L}_{\text{class}}\) and \(\mathcal{L}_{\text{spatial}}\), the model effectively learns spatio-temporal features, enabling accurate predictions of both event timing and spatial location.



\section{Experiments}
\subsection{Implementation Details}
\subsection{Training Strategy}
\subsection{Evaluation Metrics}

\section{CONCLUSIONS}
\section*{Acknowledgment}

\bibliographystyle{IEEEtran}
\bibliography{references}
\end{document}
